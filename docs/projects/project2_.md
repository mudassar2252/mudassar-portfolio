# ğŸ¤– PPO on CartPole-v1

This project involves implementing the **Proximal Policy Optimization (PPO)** algorithm from scratch using **PyTorch** and **Gymnasium** to solve the CartPole-v1 environment.

- **ğŸ§  Algorithm:** PPO (Proximal Policy Optimization)
- **ğŸ› ï¸ Tech Stack:** Python, PyTorch, Gymnasium
- **ğŸ“ GitHub Repository:** [View on GitHub](https://github.com/mudassar2252/Reinforcement)

---

## ğŸ¯ Objectives

- Understand and implement PPO.
- Apply PPO to the CartPole-v1 task.
- Analyze learning behavior and compare with other methods (e.g., A2C, DQN).
  
---

## ğŸ§ª Key Features

- **Actor-Critic Architecture:** Two neural networks (policy + value function).
- **Clipped Objective:** Prevents destructive policy updates.
- **Advantage Estimation:** Improves learning direction and efficiency.
- **Entropy Bonus:** Optional, encourages better exploration.

---

## ğŸ—ï¸ Implementation Highlights

### ğŸ”¹ Network Structure

- Input: 4 neurons (CartPole state features)
- Hidden Layers: 2 Ã— 128 neurons (ReLU)
- Output: 2 neurons (actor) / 1 neuron (critic)

### ğŸ”¹ Training Pipeline

1. Collect episode trajectories  
2. Compute discounted rewards and advantages  
3. Apply PPO updates with clipped surrogate objective  
4. Repeat for 500 episodes or until convergence

---

## ğŸ“ˆ Results

- **Converged around:** Episode 140â€“160  
- **Average Reward:** Surpassed 400  
- **Reward Plot:** Smooth with minimal oscillation  
- **Max Y-Axis:** Customized to 2500 for clarity  

---

## ğŸ† PPO Advantages

- âœ… **Fast Convergence**
- âœ… **Stable Training**
- âœ… **Simple Tuning**
- âœ… **Effective Policy Learning**

---

## ğŸ“Œ Conclusion

PPO effectively solved the CartPole-v1 problem with better speed and stability than A2C or DQN. This project enhanced understanding of modern policy-gradient methods and demonstrated PPOâ€™s strength in real-world environments.

